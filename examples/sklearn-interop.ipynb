{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interoperability with sklearn\n",
    "In this notebook, we demonstrate the interoperability of arboretum with sklearn.model_selection for cross-validation and parameter search. We will also use an example involving feature selection and a pipeline. We will be working with the ALS dataset. This is a wide noisy dataset that tree models struggle with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RFRegressor(min_leaf=5, n_trees=100, max_features=None, max_depth=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from arboretum.datasets import load_als\n",
    "from arboretum import RFRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "xtr, ytr, xte, yte = load_als()\n",
    "rf = RandomForestRegressor(n_estimators=100, min_samples_leaf=5)\n",
    "rf.fit(xtr, ytr)\n",
    "myrf = RFRegressor(n_trees=100, min_leaf=5)\n",
    "myrf.fit(xtr, ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.25643977501041193, 0.26140719623082576)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = rf.predict(xte)\n",
    "mypred = myrf.predict(xte)\n",
    "mse(yte, pred), mse(yte, mypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search CV\n",
    "Next, we run a one-parameter grid search for these models on the minimium leaf size. In order to speed things up in the notebook, we'll limit the maximum number of features tried to 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.27238839781844454, -0.2686713399674493, {'min_samples_leaf': 1})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.max_features = 30\n",
    "params = {'min_samples_leaf':[1, 5, 10, 20]}\n",
    "gcv = GridSearchCV(rf, params, 'neg_mean_squared_error')\n",
    "gcv.fit(xtr, ytr)\n",
    "pred = gcv.predict(xte)\n",
    "mse(yte, pred), gcv.best_score_, gcv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.27147188893224822, -0.26986941019167443, {'min_leaf': 10})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrf.max_features = 30\n",
    "myparams = {'min_leaf':[1, 5, 10, 20]}\n",
    "mygcv = GridSearchCV(myrf, myparams, 'neg_mean_squared_error')\n",
    "mygcv.fit(xtr, ytr)\n",
    "mypred = mygcv.predict(xte)\n",
    "mse(yte, mypred), mygcv.best_score_, mygcv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline/Feature Selection\n",
    "Next we'll set up a pipeline with a simple univariate feature selection method, and our model. We'll set the models back to using all features now that feature selection is being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26722016810894289"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "rf.max_features = None\n",
    "skb = SelectKBest(f_regression, k=30)\n",
    "pipe = Pipeline([('select', skb), ('model', rf)])\n",
    "pipe.fit(xtr, ytr)\n",
    "pred = pipe.predict(xte)\n",
    "mse(yte, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26832628758585897"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrf.max_features = None\n",
    "mypipe = Pipeline([('select', skb), ('model', myrf)])\n",
    "mypipe.fit(xtr, ytr)\n",
    "mypred = mypipe.predict(xte)\n",
    "mse(yte, mypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "A lot of the value of scikit-learn is in the 'plumbing' code for repetitive tasks like cross-validation, evaluation, and feature selection. In this notebook, we showed how to use arboretum with these parts of sklearn."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tree_dev]",
   "language": "python",
   "name": "conda-env-tree_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
